{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import functions from the scipy.signal module for signal processing\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "import scipy.signal as signal\n",
    "from scipy.fft import fft, fftfreq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carga el clasificador Haarcascade preentrenado para detección de caras\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Carga el video desde tu computadora, asegurate de que el video esté en la misma carpeta que este archivo y cambia el nombre del video para el archivo que quieras analizar\n",
    "video_filename = \"Juan60.mp4\"\n",
    "# Abre el video\n",
    "cap = cv2.VideoCapture(video_filename)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fs = round(fps)\n",
    "print(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "else:\n",
    "    # Ir al fotograma número 30\n",
    "    frame_num = 30 - 1\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "\n",
    "    # Lee el fotograma\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error al leer el fotograma del video\")\n",
    "    else:\n",
    "        # Convierte la imagen de BGR a RGB para matplotlib\n",
    "        bordered_frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Mostrar la imagen con matplotlib\n",
    "        plt.imshow(bordered_frame_rgb)\n",
    "        plt.title('Original Frame')  # Añadir el título\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Ir al fotograma número 30\n",
    "frame_num = 30 - 1  # Los índices de los fotogramas comienzan en 0\n",
    "\n",
    "# Establece la posición actual del archivo de video en el fotograma número 30\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "\n",
    "# Lee el fotograma\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Comprueba si el fotograma se ha leído correctamente\n",
    "if not ret:\n",
    "    print(\"Error al leer el fotograma del video\")\n",
    "else:\n",
    "    # Convertir a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar los rostros en la imagen\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Dibuja un rectángulo alrededor de cada rostro detectado\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 5)\n",
    "\n",
    "    # Convierte la imagen de BGR a RGB para matplotlib\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Mostrar la imagen con matplotlib para agregar el título\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.title('Original Frame with Face Detection')  # Añadir el título\n",
    "    plt.show()\n",
    "\n",
    "# Libera el capturador de video\n",
    "#cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Ir al fotograma número 30\n",
    "frame_num = 30 - 1  # Los índices de los fotogramas comienzan en 0\n",
    "\n",
    "# Establece la posición actual del archivo de video en el fotograma número 30\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "\n",
    "# Lee el fotograma\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Comprueba si el fotograma se ha leído correctamente\n",
    "if not ret:\n",
    "    print(\"Error al leer el fotograma del video\")\n",
    "else:\n",
    "    # Convertir a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar los rostros en la imagen\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Asumimos que hay al menos un rostro y tomamos el primero para recortar\n",
    "    if len(faces) > 0:\n",
    "        # Coordenadas del rostro detectado\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        # Recorta el rostro del fotograma\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Convierte la imagen de BGR a RGB para matplotlib\n",
    "        face_crop_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Mostrar la imagen recortada con matplotlib\n",
    "        plt.imshow(face_crop_rgb)\n",
    "        plt.title('Cropped Face')  # Añadir el título\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se encontraron rostros en el fotograma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_channel, green_channel, red_channel = cv2.split(face_crop)\n",
    "\n",
    "# Crear una versión del rostro con el canal verde resaltado\n",
    "# Normalizar los canales al rango 0-1 para evitar la saturación\n",
    "green_channel_normalized = green_channel / 255.0\n",
    "red_channel_normalized = red_channel / 255.0\n",
    "blue_channel_normalized = blue_channel / 255.0\n",
    "\n",
    "# Resaltar el canal verde incrementándolo y decrementando los otros canales\n",
    "green_highlighted = cv2.merge([\n",
    "    (blue_channel_normalized * 0.5).astype(np.float32),\n",
    "    (green_channel_normalized * 1.5).astype(np.float32),  # Incrementar el verde\n",
    "    (red_channel_normalized * 0.5).astype(np.float32)\n",
    "])\n",
    "\n",
    "# Asegurar que los valores estén dentro del rango 0-1 después del procesamiento\n",
    "green_highlighted = np.clip(green_highlighted, 0, 1)\n",
    "\n",
    "# Convertir de vuelta al rango 0-255 para la visualización\n",
    "green_highlighted = (green_highlighted * 255).astype(np.uint8)\n",
    "\n",
    "# Mostrar el rostro con el canal verde resaltado\n",
    "plt.imshow(cv2.cvtColor(green_highlighted, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Green Channel Face')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Inicializar lista para almacenar la intensidad promedio del canal verde del rostro en cada fotograma\n",
    "green_values = []\n",
    "\n",
    "# Establecer el número de fotogramas a analizar (puedes ajustar este valor)\n",
    "num_frames_to_analyze = 300  # Por ejemplo, analizar 300 fotogramas\n",
    "\n",
    "# Leer el video fotograma a fotograma\n",
    "for _ in range(num_frames_to_analyze):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Si no quedan fotogramas, sal del bucle\n",
    "\n",
    "    # Convertir a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        # Tomar el primer rostro detectado\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        # Extraer la región del rostro\n",
    "        face_region = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Calcular la intensidad promedio del canal verde en la región del rostro\n",
    "        green_channel = face_region[:, :, 1]\n",
    "        green_value = np.mean(green_channel)\n",
    "        green_values.append(green_value)\n",
    "\n",
    "# Convertir la lista de valores verdes a un array de numpy\n",
    "green_array = np.array(green_values)\n",
    "\n",
    "# Mostrar la señal del canal verde\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(green_array, color='green')\n",
    "plt.title('Green Channel Intensity Over Time in the Face Region')\n",
    "plt.xlabel('Frame number')\n",
    "plt.ylabel('Average Green Intensity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para crear un filtro pasa banda\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "# Función para aplicar el filtro pasa banda\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Lista para almacenar los valores medios del canal verde\n",
    "green_values = []\n",
    "\n",
    "# Establecer la frecuencia de muestreo (fps del video)\n",
    "fs = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Leer el video fotograma a fotograma\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]\n",
    "        face_region = frame[y:y+h, x:x+w]\n",
    "        green_channel = face_region[:, :, 1]\n",
    "        green_value = np.mean(green_channel)\n",
    "        green_values.append(green_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la lista de valores verdes a un array de numpy\n",
    "green_array = np.array(green_values)\n",
    "\n",
    "# Filtrar la señal para eliminar el ruido y enfocarse en las frecuencias del pulso cardíaco\n",
    "lowcut = 1.0  # Frecuencia mínima del pulso (en Hz)\n",
    "highcut = 1.6  # Frecuencia máxima del pulso (en Hz)\n",
    "filtered_green = butter_bandpass_filter(green_array, lowcut, highcut, fs, order=5)\n",
    "\n",
    "# Realizar la FFT en la señal filtrada\n",
    "green_fft = fft(filtered_green)\n",
    "freqs = fftfreq(len(filtered_green), 1/fs)\n",
    "\n",
    "# Encontrar la frecuencia con la magnitud más alta en el rango de la frecuencia cardíaca\n",
    "idx = np.argmax(np.abs(green_fft))\n",
    "pulse_freq = freqs[idx]\n",
    "heart_rate = pulse_freq * 60  # Convertir a latidos por minuto\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f'Estimated Heart Rate: {heart_rate:.1f} beats per minute')\n",
    "\n",
    "# Mostrar la señal temporal filtrada y su FFT\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Señal temporal filtrada\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(filtered_green, color='green')\n",
    "plt.title('Filtered Green Channel Signal')\n",
    "\n",
    "# FFT de la señal\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.stem(freqs, np.abs(green_fft), 'b', markerfmt=\" \", basefmt=\"-b\")\n",
    "plt.title('Spectral Analysis')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlim(0, highcut * 2)  # Limitar el eje x para una mejor visualización\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar la FFT en la señal filtrada\n",
    "green_fft = fft(filtered_green)\n",
    "freqs = fftfreq(len(filtered_green), 1/fs)\n",
    "\n",
    "# Encontrar la frecuencia con la magnitud más alta en el rango de la frecuencia cardíaca\n",
    "idx = np.argmax(np.abs(green_fft))\n",
    "pulse_freq = freqs[idx]\n",
    "heart_rate = pulse_freq * 60  # Convertir a latidos por minuto\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f'Estimated Heart Rate: {heart_rate:.1f} beats per minute')\n",
    "\n",
    "# Mostrar la señal temporal filtrada y su FFT\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Señal temporal filtrada\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(filtered_green, color='green')\n",
    "plt.title('Filtered Green Channel Signal')\n",
    "\n",
    "# FFT de la señal\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.stem(freqs, np.abs(green_fft), 'b', markerfmt=\" \", basefmt=\"-b\")\n",
    "plt.scatter(pulse_freq, np.abs(green_fft[idx]), color='red')  # Punto para la frecuencia dominante\n",
    "\n",
    "# Ajustar la posición del texto\n",
    "# Aquí puedes ajustar las coordenadas para mover el texto\n",
    "text_x = pulse_freq + 0.1  # Ajusta este valor según sea necesario\n",
    "text_y = np.abs(green_fft[idx]) - 50  # Ajusta este valor según sea necesario\n",
    "plt.text(text_x, text_y, f'{pulse_freq:.2f} Hz', color='red')\n",
    "\n",
    "plt.title('Spectral Analysis')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlim(0, highcut * 2)  # Limitar el eje x para una mejor visualización\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, graficamos la intensidad del canal verde\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(green_array, color='green')\n",
    "plt.title('Green Channel Intensity Over Time')\n",
    "plt.xlabel('Frame number')\n",
    "plt.ylabel('Average Green Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Lista para almacenar los valores medios del canal verde\n",
    "green_values = []\n",
    "\n",
    "# Procesar solo los primeros 30 fotogramas\n",
    "for i in range(30):\n",
    "    # Leer el fotograma\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error al leer el fotograma del video\")\n",
    "        break\n",
    "\n",
    "    # Convertir el fotograma a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Verificar si se encontró al menos un rostro\n",
    "    if len(faces) > 0:\n",
    "        # Tomar el primer rostro detectado\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        # Extraer la región del rostro\n",
    "        face_region = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Extraer el canal verde de la región del rostro\n",
    "        green_channel = face_region[:, :, 1]\n",
    "\n",
    "        # Normalizar el canal verde para mejorar la visualización\n",
    "        green_channel_normalized = cv2.normalize(green_channel, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "        # Añadir el valor medio del canal verde normalizado a la lista\n",
    "        green_values.append(np.mean(green_channel_normalized))\n",
    "\n",
    "        # Crear una imagen en escala de grises con el canal verde como intensidad\n",
    "        green_visualization = np.stack((green_channel_normalized,) * 3, axis=-1)\n",
    "\n",
    "        # Visualizar el canal verde en el rostro detectado\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(cv2.cvtColor(green_visualization, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Green Channel in Face Region - Frame {i+1}')\n",
    "        plt.axis('off')  # Ocultar los ejes\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se encontró ningún rostro en el fotograma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Procesar solo los primeros 30 fotogramas\n",
    "for i in range(30):\n",
    "    # Leer el fotograma\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error al leer el fotograma del video\")\n",
    "        break\n",
    "\n",
    "    # Convertir el fotograma a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Verificar si se encontró al menos un rostro\n",
    "    if len(faces) > 0:\n",
    "        # Tomar el primer rostro detectado\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        # Extraer la región del rostro\n",
    "        face_region = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Extraer el canal verde de la región del rostro\n",
    "        green_channel = face_region[:, :, 1]\n",
    "        print(green_channel.shape)\n",
    "        # Escalar los valores del canal verde para maximizar la visibilidad\n",
    "        max_green = green_channel.max()\n",
    "        if max_green > 0:  # Evitar la división por cero\n",
    "            scale_factor = 255 / max_green\n",
    "            green_channel_scaled = (green_channel * scale_factor).astype(np.uint8)\n",
    "        else:\n",
    "            green_channel_scaled = green_channel\n",
    "\n",
    "        # Crear una imagen en escala de grises con el canal verde maximizado como intensidad\n",
    "        green_maximized_visualization = np.zeros_like(face_region)\n",
    "        green_maximized_visualization[:, :, 1] = green_channel_scaled\n",
    "\n",
    "        # Visualizar el canal verde maximizado en el rostro detectado\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(cv2.cvtColor(green_maximized_visualization, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Green Channel Maximized in Face Region - Frame {i+1}')\n",
    "        plt.axis('off')  # Ocultar los ejes\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se encontró ningún rostro en el fotograma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Procesar solo los primeros 30 fotogramas\n",
    "for i in range(30):\n",
    "    # Leer el fotograma\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error al leer el fotograma del video\")\n",
    "        break\n",
    "\n",
    "    # Convertir el fotograma a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Verificar si se encontró al menos un rostro\n",
    "    if len(faces) > 0:\n",
    "        # Tomar el primer rostro detectado\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        # Extraer la región del rostro\n",
    "        face_region = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Extraer el canal verde de la región del rostro\n",
    "        green_channel = face_region[:, :, 1]\n",
    "\n",
    "        # Normalizar el canal verde para mejorar la visualización\n",
    "        green_channel_normalized = cv2.normalize(green_channel, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        # Crear una imagen en escala de grises con el canal verde como intensidad\n",
    "        green_visualization = np.zeros_like(face_region)\n",
    "        green_visualization[:, :, 1] = np.uint8(green_channel_normalized * 255)  # Coloca el canal verde en la imagen\n",
    "\n",
    "        # Visualizar el canal verde en el rostro detectado\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(cv2.cvtColor(green_visualization, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Green Channel in Face Region - Frame {i+1}')\n",
    "        plt.axis('off')  # Ocultar los ejes\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se encontró ningún rostro en el fotograma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Procesar solo los primeros 30 fotogramas\n",
    "for i in range(30):\n",
    "    # Leer el fotograma\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error al leer el fotograma del video\")\n",
    "        break\n",
    "\n",
    "    # Convertir el fotograma a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Verificar si se encontró al menos un rostro\n",
    "    if len(faces) > 0:\n",
    "        # Tomar el primer rostro detectado\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        # Definir una ROI para la mejilla (ajustar según sea necesario)\n",
    "        # Estos valores son estimaciones y podrían necesitar ajuste fino\n",
    "        mejilla_x = x + int(w * 0.3)  # Comienza un poco a la derecha del lado izquierdo del rostro\n",
    "        mejilla_y = y + int(h * 0.4)  # Comienza un poco debajo de la mitad del alto del rostro\n",
    "        mejilla_w = int(w * 0.4)  # Ancho de la mejilla\n",
    "        mejilla_h = int(h * 0.2)  # Alto de la mejilla\n",
    "\n",
    "        # Extraer la región de la mejilla\n",
    "        mejilla_region = frame[mejilla_y:mejilla_y+mejilla_h, mejilla_x:mejilla_x+mejilla_w]\n",
    "\n",
    "        # Extraer el canal verde de la región de la mejilla\n",
    "        green_channel = mejilla_region[:, :, 1]\n",
    "\n",
    "        # Normalizar el canal verde para mejorar la visualización\n",
    "        green_channel_normalized = cv2.normalize(green_channel, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        # Visualizar el canal verde en la región de la mejilla\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(cv2.cvtColor(mejilla_region, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Green Channel in Cheek Region - Frame {i+1}')\n",
    "        plt.axis('off')  # Ocultar los ejes\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se encontró ningún rostro en el fotograma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que el video se ha abierto correctamente\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el archivo de video\")\n",
    "\n",
    "# Procesar solo los primeros 30 fotogramas\n",
    "for i in range(30):\n",
    "    # Leer el fotograma\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error al leer el fotograma del video\")\n",
    "        break\n",
    "\n",
    "    # Convertir el fotograma a escala de grises para la detección de rostros\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    # Verificar si se encontró al menos un rostro\n",
    "    if len(faces) > 0:\n",
    "        # Tomar el primer rostro detectado\n",
    "        x, y, w, h = faces[0]\n",
    "\n",
    "        # Ajustar la ubicación de la ROI para enfocarse en la mejilla\n",
    "        mejilla_x = x + int(w * 0.6)  # Ajustar hacia el lado derecho del rostro\n",
    "        mejilla_y = y + int(h * 0.4)  # Ajustar ligeramente hacia abajo\n",
    "        mejilla_w = int(w * 0.3)  # Ancho más estrecho\n",
    "        mejilla_h = int(h * 0.3)  # Alto\n",
    "\n",
    "        # Extraer la región de la mejilla\n",
    "        mejilla_region = frame[mejilla_y:mejilla_y+mejilla_h, mejilla_x:mejilla_x+mejilla_w]\n",
    "\n",
    "        # Extraer el canal verde de la región de la mejilla y normalizarlo\n",
    "        green_channel = mejilla_region[:, :, 1]\n",
    "        green_channel_normalized = cv2.normalize(green_channel, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        # Crear una imagen en escala de grises con el canal verde como intensidad\n",
    "        green_visualization = np.zeros_like(mejilla_region)\n",
    "        green_visualization[:, :, 1] = np.uint8(green_channel_normalized * 255)  # Coloca el canal verde en la imagen\n",
    "\n",
    "        # Visualizar el canal verde en la región de la mejilla\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(cv2.cvtColor(green_visualization, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f'Green Channel in Cheek Region - Frame {i+1}')\n",
    "        plt.axis('off')  # Ocultar los ejes\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se encontró ningún rostro en el fotograma.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
