{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "class CameraApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        self.window.title(\"Camera App\")\n",
    "\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        self.is_recording = False\n",
    "\n",
    "        self.canvas = tk.Canvas(window, width=640, height=480)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        self.record_button = tk.Button(window, text=\"Record\", command=self.toggle_recording, relief=tk.RAISED, bd=3, padx=10, pady=5, font=(\"Arial\", 12), bg=\"lightblue\", fg=\"black\", activebackground=\"lightgreen\", activeforeground=\"black\")\n",
    "        self.record_button.pack()\n",
    "\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame)\n",
    "            photo = ImageTk.PhotoImage(image)\n",
    "            self.canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "            self.canvas.image = photo\n",
    "\n",
    "        self.window.after(15, self.update)\n",
    "\n",
    "    def toggle_recording(self):\n",
    "        self.is_recording = not self.is_recording\n",
    "        if self.is_recording:\n",
    "            # Start recording\n",
    "            self.record_button.config(text=\"Stop Recording\")\n",
    "        else:\n",
    "            # Stop recording\n",
    "            self.record_button.config(text=\"Record\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    window = tk.Tk()\n",
    "    app = CameraApp(window)\n",
    "    window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "class CameraApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        self.window.title(\"Camera App\")\n",
    "        self.window.configure(background='black')  # Fondo de la ventana en negro\n",
    "\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        self.is_recording = False\n",
    "\n",
    "        self.canvas = tk.Canvas(window, width=640, height=480, bg='black', highlightthickness=0)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        # Personalización del botón con fondo y texto para contraste y legibilidad\n",
    "        self.record_button = tk.Button(window, text=\"Record\", command=self.toggle_recording,\n",
    "                                       relief=tk.FLAT, bd=0, padx=10, pady=5, font=(\"Arial\", 12),\n",
    "                                       bg=\"black\", fg=\"white\", activebackground=\"grey\", activeforeground=\"white\")\n",
    "        self.record_button.pack()\n",
    "\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame)\n",
    "            photo = ImageTk.PhotoImage(image)\n",
    "            self.canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "            self.canvas.image = photo\n",
    "\n",
    "        self.window.after(15, self.update)\n",
    "\n",
    "    def toggle_recording(self):\n",
    "        self.is_recording = not self.is_recording\n",
    "        if self.is_recording:\n",
    "            # Start recording\n",
    "            self.record_button.config(text=\"Stop Recording\")\n",
    "        else:\n",
    "            # Stop recording\n",
    "            self.record_button.config(text=\"Record\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    window = tk.Tk()\n",
    "    app = CameraApp(window)\n",
    "    window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "class CameraApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        self.window.title(\"Camera App\")\n",
    "        self.window.configure(background='black')  # Fondo de la ventana en negro\n",
    "\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        self.is_recording = False\n",
    "\n",
    "        self.canvas = tk.Canvas(window, width=640, height=480, bg='black', highlightthickness=0)\n",
    "        self.canvas.pack()\n",
    "\n",
    "        self.record_button = tk.Button(window, text=\"Start Camera\", command=self.toggle_recording,\n",
    "                                       relief=tk.FLAT, bd=0, padx=10, pady=5, font=(\"Arial\", 12),\n",
    "                                       bg=\"black\", fg=\"white\", activebackground=\"grey\", activeforeground=\"white\")\n",
    "        self.record_button.pack()\n",
    "\n",
    "        self.silhouette_image = Image.open(\"user.png\")  # Asegúrate de que esta ruta sea correcta\n",
    "        self.silhouette_photo = ImageTk.PhotoImage(self.silhouette_image)\n",
    "        self.canvas.create_image(320, 240, image=self.silhouette_photo, anchor=tk.CENTER)\n",
    "\n",
    "        self.update()\n",
    "\n",
    "    def update(self):\n",
    "        if self.is_recording:\n",
    "            ret, frame = self.video_capture.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image = Image.fromarray(frame)\n",
    "                photo = ImageTk.PhotoImage(image)\n",
    "                self.canvas.create_image(0, 0, image=photo, anchor=tk.NW)\n",
    "                self.canvas.image = photo\n",
    "        else:\n",
    "            # Mostrar la silueta cuando no se está grabando\n",
    "            self.canvas.create_image(320, 240, image=self.silhouette_photo, anchor=tk.CENTER)\n",
    "            \n",
    "        self.window.after(15, self.update)\n",
    "\n",
    "    def toggle_recording(self):\n",
    "        self.is_recording = not self.is_recording\n",
    "        if self.is_recording:\n",
    "            self.record_button.config(text=\"Stop Camera\")\n",
    "        else:\n",
    "            self.record_button.config(text=\"Start Camera\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    window = tk.Tk()\n",
    "    app = CameraApp(window)\n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detector de puntos de la cara\n",
    "# import the necessary packages\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    " \n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(p)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while True:\n",
    "    # load the input image and convert it to grayscale\n",
    "    _, image = cap.read()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "    \n",
    "    # show the output image with the face detections + facial landmarks\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar el detector de rostros de dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Iniciar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir a escala de grises para la detección\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        # Dibuja un rectángulo alrededor del rostro\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    cv2.imshow(\"Webcam Face Detection\", frame)\n",
    "\n",
    "    # Presiona 'q' para cerrar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar el detector de rostros de dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "# Cargar el predictor de puntos faciales\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Iniciar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir a escala de grises para la detección\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        \n",
    "        # Obtener puntos faciales para la cara detectada\n",
    "        landmarks = predictor(gray, face)\n",
    "        \n",
    "        # Crear una lista de puntos de los contornos del rostro\n",
    "        points = []\n",
    "        for n in range(0, 68):  # El predictor tiene 68 puntos\n",
    "            x_point = landmarks.part(n).x\n",
    "            y_point = landmarks.part(n).y\n",
    "            points.append((x_point, y_point))\n",
    "        \n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        mask = np.zeros_like(frame)\n",
    "\n",
    "        # Rellenar la máscara con el color verde solo en los contornos del rostro\n",
    "        cv2.fillConvexPoly(mask, points, (0, 255, 0))\n",
    "        \n",
    "        # Aplicar la máscara sobre el frame original\n",
    "        frame = cv2.addWeighted(frame, 1, mask, 0.4, 0)  # Ajusta 0.4 para la transparencia\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    cv2.imshow(\"Webcam Face Detection - Face Paint\", frame)\n",
    "\n",
    "    # Presiona 'q' para cerrar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Máscara casi completamente conseguida\n",
    "# Cargar el detector de rostros de dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "# Cargar el predictor de puntos faciales\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Asegúrate de tener el modelo correcto\n",
    "\n",
    "# Iniciar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir a escala de grises para la detección\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Seleccionar los puntos de los contornos de la cara y el mentón\n",
    "        face_outline_points = [landmarks.part(i) for i in range(17)] + [landmarks.part(i) for i in range(26, 16, -1)]\n",
    "\n",
    "        # Convertir a np.array\n",
    "        face_outline = np.array([[p.x, p.y] for p in face_outline_points], dtype=np.int32)\n",
    "\n",
    "        # Crear una máscara del mismo tamaño que el frame\n",
    "        mask = np.zeros_like(frame)\n",
    "\n",
    "        # Rellenar el contorno del rostro en la máscara con el color verde\n",
    "        cv2.fillPoly(mask, [face_outline], (0, 255, 0))\n",
    "\n",
    "        # Suavizar los bordes de la máscara\n",
    "        mask = cv2.GaussianBlur(mask, (0, 0), sigmaX=5, sigmaY=5, borderType=cv2.BORDER_DEFAULT)\n",
    "\n",
    "        # Crear la máscara para el área de la cara (sin ojos y boca)\n",
    "        mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "        # Aplicar la máscara al frame original para borrar la cara\n",
    "        frame_no_face = cv2.bitwise_and(frame, mask_inv)\n",
    "\n",
    "        # Aplicar la máscara verde a la región de la cara\n",
    "        frame_face_green = cv2.bitwise_and(mask, (0, 255, 0, 0))\n",
    "\n",
    "        # Combinar ambas imágenes\n",
    "        frame = cv2.add(frame_no_face, frame_face_green)\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    cv2.imshow(\"Webcam Face Detection - Green Face\", frame)\n",
    "\n",
    "    # Presiona 'q' para cerrar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Detectar rostros\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     21\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m predictor(gray, face)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#Máscara dejando los ojos fuera del coloreado\n",
    "# Cargar el detector de rostros de dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "# Cargar el predictor de puntos faciales\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Asegúrate de tener el modelo correcto\n",
    "\n",
    "# Iniciar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir a escala de grises para la detección\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Crear una máscara del mismo tamaño que el frame para el rostro\n",
    "        mask_face = np.zeros_like(frame)\n",
    "        # Crear una máscara para los ojos\n",
    "        mask_eyes = np.zeros_like(frame)\n",
    "\n",
    "        # Seleccionar los puntos de los contornos de la cara y el mentón\n",
    "        face_outline_points = [landmarks.part(i) for i in range(17)] + [landmarks.part(i) for i in range(26, 16, -1)]\n",
    "        # Convertir a np.array\n",
    "        face_outline = np.array([[p.x, p.y] for p in face_outline_points], dtype=np.int32)\n",
    "\n",
    "        # Seleccionar los puntos alrededor de los ojos\n",
    "        eye_points = np.concatenate([np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in range(36, 42)]), # Ojo izquierdo\n",
    "                                     np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in range(42, 48)])]) # Ojo derecho\n",
    "\n",
    "        # Rellenar el contorno del rostro en la máscara con el color verde\n",
    "        cv2.fillPoly(mask_face, [face_outline], (0, 255, 0))\n",
    "        # Rellenar los ojos en la máscara para los ojos\n",
    "        cv2.fillPoly(mask_eyes, [eye_points[:6]], (255, 255, 255))\n",
    "        cv2.fillPoly(mask_eyes, [eye_points[6:]], (255, 255, 255))\n",
    "\n",
    "        # Suavizar los bordes de la máscara del rostro\n",
    "        mask_face = cv2.GaussianBlur(mask_face, (0, 0), sigmaX=5, sigmaY=5, borderType=cv2.BORDER_DEFAULT)\n",
    "\n",
    "        # Combinar las máscaras para excluir los ojos\n",
    "        mask_combined = cv2.subtract(mask_face, mask_eyes)\n",
    "\n",
    "        # Crear la máscara inversa para la cara\n",
    "        mask_face_inv = cv2.bitwise_not(mask_combined)\n",
    "\n",
    "        # Aplicar la máscara al frame original para borrar la cara pero dejar los ojos\n",
    "        frame_no_face = cv2.bitwise_and(frame, mask_face_inv)\n",
    "\n",
    "        # Aplicar la máscara verde a la región de la cara\n",
    "        frame_face_green = cv2.bitwise_and(mask_combined, (0, 255, 0, 0))\n",
    "\n",
    "        # Combinar ambas imágenes\n",
    "        frame = cv2.add(frame_no_face, frame_face_green)\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    cv2.imshow(\"Webcam Face Detection - Green Mask\", frame)\n",
    "\n",
    "    # Presiona 'q' para cerrar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cargar el detector de rostros de dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "# Cargar el predictor de puntos faciales\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Asegúrate de tener el modelo correcto\n",
    "\n",
    "# Iniciar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir a escala de grises para la detección\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostros\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Crear una máscara del mismo tamaño que el frame para el rostro, los ojos y la boca\n",
    "        mask_face = np.zeros_like(frame)\n",
    "        mask_eyes = np.zeros_like(frame)\n",
    "        mask_mouth = np.zeros_like(frame)\n",
    "\n",
    "        # Seleccionar los puntos de los contornos de la cara y el mentón\n",
    "        face_outline_points = [landmarks.part(i) for i in range(17)] + [landmarks.part(i) for i in range(26, 16, -1)]\n",
    "        # Convertir a np.array\n",
    "        face_outline = np.array([[p.x, p.y] for p in face_outline_points], dtype=np.int32)\n",
    "\n",
    "        # Seleccionar los puntos alrededor de los ojos\n",
    "        eye_points = np.concatenate([np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in range(36, 42)]), # Ojo izquierdo\n",
    "                                     np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in range(42, 48)])]) # Ojo derecho\n",
    "        \n",
    "        # Seleccionar los puntos alrededor del contorno exterior de la boca\n",
    "        mouth_points = np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in range(48, 60)]) # Boca\n",
    "\n",
    "        # Rellenar el contorno del rostro en la máscara con el color verde\n",
    "        cv2.fillPoly(mask_face, [face_outline], (0, 255, 0))\n",
    "\n",
    "        # Rellenar los ojos en la máscara para los ojos\n",
    "        cv2.fillPoly(mask_eyes, [eye_points[:6]], (255, 255, 255))\n",
    "        cv2.fillPoly(mask_eyes, [eye_points[6:]], (255, 255, 255))\n",
    "\n",
    "        # Rellenar la boca en la máscara para la boca\n",
    "        cv2.fillPoly(mask_mouth, [mouth_points], (255, 255, 255))\n",
    "\n",
    "        # Suavizar los bordes de la máscara del rostro\n",
    "        mask_face = cv2.GaussianBlur(mask_face, (0, 0), sigmaX=5, sigmaY=5, borderType=cv2.BORDER_DEFAULT)\n",
    "\n",
    "        # Combinar las máscaras de los ojos y la boca\n",
    "        mask_eyes_mouth = cv2.bitwise_or(mask_eyes, mask_mouth)\n",
    "\n",
    "        # Combinar las máscaras para excluir los ojos y la boca\n",
    "        mask_combined = cv2.subtract(mask_face, mask_eyes_mouth)\n",
    "\n",
    "        # Crear la máscara inversa para la cara\n",
    "        mask_face_inv = cv2.bitwise_not(mask_combined)\n",
    "\n",
    "        # Aplicar la máscara al frame original para borrar la cara pero dejar los ojos y la boca\n",
    "        frame_no_face = cv2.bitwise_and(frame, mask_face_inv)\n",
    "\n",
    "        # Aplicar la máscara verde a la región de la cara\n",
    "        frame_face_green = cv2.bitwise_and(mask_combined, (250, 0, 0, 0))\n",
    "\n",
    "        # Combinar ambas imágenes\n",
    "        frame = cv2.add(frame_no_face, frame_face_green)\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    cv2.imshow(\"Webcam Face Detection - Green Mask with Eyes and Mouth\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar la cámara y cerrar todas las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codigo para calcular el HR en video directo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Checar si la cámara fue abierta correctamente\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"No se puede abrir la webcam\")\n",
    "\n",
    "def calcular_promedio_verde(frame):\n",
    "    # Extraer el canal verde\n",
    "    green_channel = frame[:, :, 1]\n",
    "    # Calcular el promedio del canal verde\n",
    "    return np.mean(green_channel)\n",
    "\n",
    "def procesar_señal(valores_verde, fps):\n",
    "    # Filtrar la señal para reducir ruido\n",
    "    b, a = butter(3, (1/(fps/2), 1.6/(fps/2)), btype='band')\n",
    "    filtered_signal = filtfilt(b, a, valores_verde)\n",
    "    # Detectar picos en la señal filtrada\n",
    "    peaks, _ = find_peaks(filtered_signal, distance=fps/2)\n",
    "    # Calcular ritmo cardíaco\n",
    "    if len(peaks) > 1:\n",
    "        intervalos = np.diff(peaks) / fps\n",
    "        ritmo_cardiaco = 60 / np.mean(intervalos)\n",
    "    else:\n",
    "        ritmo_cardiaco = 0\n",
    "    return ritmo_cardiaco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    valores_verde = []\n",
    "    start_time = time.time()\n",
    "    fps = 30  # Asumiendo un FPS de 30 para el procesamiento\n",
    "    ritmo_cardiaco = 0  # Inicializar ritmo cardíaco\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Calcular el promedio del canal verde\n",
    "        promedio_verde = calcular_promedio_verde(frame)\n",
    "        valores_verde.append(promedio_verde)\n",
    "\n",
    "        # Procesar la señal cada 15 segundos para estimar el ritmo cardíaco\n",
    "        if time.time() - start_time > 15:\n",
    "            ritmo_cardiaco = procesar_señal(valores_verde, fps)\n",
    "            print(f\"Ritmo cardíaco estimado: {ritmo_cardiaco:.2f} bpm\")\n",
    "            valores_verde = []  # Reiniciar la lista de valores verdes\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Mostrar el frame actual y el ritmo cardíaco\n",
    "        cv2.putText(frame, f\"Ritmo Cardiaco: {ritmo_cardiaco:.2f} bpm\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow('Webcam', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_detector_rostros():\n",
    "    # Cargar el clasificador preentrenado para rostros (Haar Cascade)\n",
    "    return cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "def detectar_rostro(frame, detector):\n",
    "    # Convertir a escala de grises para la detección\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Detectar rostros en el frame\n",
    "    rostros = detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    if len(rostros) == 0:\n",
    "        return None  # No se encontró ningún rostro\n",
    "    # Seleccionar el rostro más grande detectado (asumiendo que es el principal)\n",
    "    x, y, w, h = max(rostros, key=lambda b: b[2] * b[3])\n",
    "    return frame[y:y+h, x:x+w]\n",
    "\n",
    "def calcular_promedio_verde(frame):\n",
    "    # Extraer el canal verde del área del rostro\n",
    "    green_channel = frame[:, :, 1]\n",
    "    # Calcular el promedio del canal verde\n",
    "    return np.mean(green_channel)\n",
    "\n",
    "def procesar_señal(valores_verde, fps):\n",
    "    # Filtrar la señal para reducir ruido\n",
    "    b, a = butter(3, (1/(fps/2), 1.6/(fps/2)), btype='band')\n",
    "    filtered_signal = filtfilt(b, a, valores_verde)\n",
    "    # Detectar picos en la señal filtrada\n",
    "    peaks, _ = find_peaks(filtered_signal, distance=fps/2)\n",
    "    # Calcular ritmo cardíaco\n",
    "    if len(peaks) > 1:\n",
    "        intervalos = np.diff(peaks) / fps\n",
    "        ritmo_cardiaco = 60 / np.mean(intervalos)\n",
    "    else:\n",
    "        ritmo_cardiaco = 0\n",
    "    return ritmo_cardiaco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"No se puede abrir la webcam\")\n",
    "    \n",
    "    fps = 30  # Asumiendo un FPS de 30 para el procesamiento\n",
    "    detector_rostros = inicializar_detector_rostros()\n",
    "    valores_verde = []\n",
    "    start_time = time.time()\n",
    "    ritmo_cardiaco = 0  # Inicializar ritmo cardíaco\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rostro = detectar_rostro(frame, detector_rostros)\n",
    "        promedio_verde = calcular_promedio_verde(rostro)\n",
    "        if promedio_verde is not None:\n",
    "            valores_verde.append(promedio_verde)\n",
    "\n",
    "        # Procesar la señal cada 15 segundos para estimar el ritmo cardíaco\n",
    "        if time.time() - start_time > 15:\n",
    "            if valores_verde:\n",
    "                ritmo_cardiaco = procesar_señal(valores_verde, fps)\n",
    "                valores_verde = []  # Reiniciar la lista de valores verdes\n",
    "            start_time = time.time()\n",
    "            print(f\"Ritmo cardíaco estimado: {ritmo_cardiaco:.2f} bpm\")\n",
    "\n",
    "        # Mostrar el frame actual y el ritmo cardíaco\n",
    "        cv2.putText(frame, f\"Ritmo Cardiaco: {ritmo_cardiaco:.2f} bpm\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow('Webcam', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcamera\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m, in \u001b[0;36mcamera\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     17\u001b[0m rostro \u001b[38;5;241m=\u001b[39m detectar_rostro(frame, detector_rostros)\n\u001b[1;32m---> 18\u001b[0m promedio_verde \u001b[38;5;241m=\u001b[39m \u001b[43mcalcular_promedio_verde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrostro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m promedio_verde \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     valores_verde\u001b[38;5;241m.\u001b[39mappend(promedio_verde)\n",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m, in \u001b[0;36mcalcular_promedio_verde\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalcular_promedio_verde\u001b[39m(frame):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Extraer el canal verde del área del rostro\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     green_channel \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Calcular el promedio del canal verde\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(green_channel)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceso de camara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'butter_bandpass_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m saturation_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(saturation_values)\n\u001b[0;32m     55\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m  \u001b[38;5;66;03m# Frecuencia de muestreo aproximada\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m filtered_saturation \u001b[38;5;241m=\u001b[39m \u001b[43mbutter_bandpass_filter\u001b[49m(saturation_array, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m4\u001b[39m, fs, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     57\u001b[0m saturation_fft \u001b[38;5;241m=\u001b[39m fft(filtered_saturation)\n\u001b[0;32m     58\u001b[0m freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftfreq(\u001b[38;5;28mlen\u001b[39m(filtered_saturation), \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mfs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'butter_bandpass_filter' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.fft import fft\n",
    "\n",
    "# Función para inicializar el detector de rostros\n",
    "def inicializar_detector_rostros():\n",
    "    return cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Función para detectar y extraer el rostro\n",
    "def detectar_rostro(frame, detector):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector.detectMultiScale(gray, 1.1, 4)\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]\n",
    "        return frame[y:y+h, x:x+w]\n",
    "    return None\n",
    "\n",
    "# Función para calcular el promedio de saturación del rostro\n",
    "def calcular_promedio_saturacion(roi):\n",
    "    if roi is None:\n",
    "        return None\n",
    "    hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    _, s, _ = cv2.split(hsv_roi)\n",
    "    return np.mean(s)\n",
    "\n",
    "# Configuración inicial\n",
    "detector_rostros = inicializar_detector_rostros()\n",
    "cap = cv2.VideoCapture(0)\n",
    "saturation_values = []\n",
    "\n",
    "# Procesar video en tiempo real\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rostro = detectar_rostro(frame, detector_rostros)\n",
    "        promedio_saturacion = calcular_promedio_saturacion(rostro)\n",
    "        if promedio_saturacion is not None:\n",
    "            saturation_values.append(promedio_saturacion)\n",
    "\n",
    "        # Mostrar el frame actual en la ventana\n",
    "        cv2.imshow('Webcam', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Análisis de los valores de saturación\n",
    "saturation_array = np.array(saturation_values)\n",
    "fs = 30  # Frecuencia de muestreo aproximada\n",
    "filtered_saturation = butter_bandpass_filter(saturation_array, 0.5, 4, fs, order=5)\n",
    "saturation_fft = fft(filtered_saturation)\n",
    "freqs = np.fft.fftfreq(len(filtered_saturation), 1/fs)\n",
    "idx = np.argmax(np.abs(saturation_fft))\n",
    "pulse_freq = freqs[idx]\n",
    "heart_rate = pulse_freq * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "\n",
    "# Función para inicializar el detector de rostros\n",
    "def inicializar_detector_rostros():\n",
    "    return cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Función para detectar el rostro en el frame\n",
    "def detectar_rostro(frame, detector):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector.detectMultiScale(gray, 1.1, 4)\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]\n",
    "        return frame[y:y+h, x:x+w]\n",
    "    return None\n",
    "\n",
    "# Función para calcular el promedio de intensidad del canal verde\n",
    "def calcular_promedio_verde(roi):\n",
    "    if roi is None:\n",
    "        return None\n",
    "    green_channel = roi[:, :, 1]\n",
    "    return np.mean(green_channel)\n",
    "\n",
    "# Función para procesar los valores y estimar el ritmo cardíaco\n",
    "def estimar_ritmo_cardiaco(valores, fps):\n",
    "    valores = np.array(valores)\n",
    "    b, a = butter(3, [0.75 / (fps / 2), 2.5 / (fps / 2)], btype='band')\n",
    "    filtered = filtfilt(b, a, valores)\n",
    "    peaks, _ = find_peaks(filtered, distance=fps/2)\n",
    "    if len(peaks) > 1:\n",
    "        heart_rate = 60 * len(peaks) / (len(filtered) / fps)\n",
    "        return heart_rate\n",
    "    return 0\n",
    "\n",
    "# Configuración inicial\n",
    "detector_rostros = inicializar_detector_rostros()\n",
    "cap = cv2.VideoCapture(0)\n",
    "valores_verde = []\n",
    "start_time = time.time()\n",
    "fps = 30  # Asumiendo un FPS de 30 para el procesamiento\n",
    "\n",
    "# Procesar video en tiempo real\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rostro = detectar_rostro(frame, detector_rostros)\n",
    "        promedio_verde = calcular_promedio_verde(rostro)\n",
    "        if promedio_verde is not None:\n",
    "            valores_verde.append(promedio_verde)\n",
    "\n",
    "        if time.time() - start_time > 15 and valores_verde:\n",
    "            ritmo_cardiaco = estimar_ritmo_cardiaco(valores_verde, fps)\n",
    "            valores_verde = []  # Reiniciar la lista de valores verdes\n",
    "            start_time = time.time()\n",
    "        else:\n",
    "            ritmo_cardiaco = 0\n",
    "\n",
    "        cv2.putText(frame, f\"Ritmo Cardiaco: {ritmo_cardiaco:.2f} bpm\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow('Webcam', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
